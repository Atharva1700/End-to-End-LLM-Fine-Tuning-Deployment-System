{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning Tutorial\n",
    "## Complete Guide to Training and Deploying Your Model\n",
    "\n",
    "This notebook walks you through the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers accelerate peft datasets tqdm\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the data generation script\n",
    "!python ../scripts/create_sample_data.py --output_dir ../data/processed\n",
    "\n",
    "# Verify data was created\n",
    "!ls -lh ../data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train with LoRA\n",
    "!python ../training/train_lora.py \\\n",
    "    --model_name \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \\\n",
    "    --dataset_path ../data/processed/train.jsonl \\\n",
    "    --output_dir ../models/tutorial_model \\\n",
    "    --num_epochs 1 \\\n",
    "    --batch_size 2 \\\n",
    "    --use_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load model\n",
    "model_path = \"../models/tutorial_model\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Test inference\n",
    "def generate_response(prompt, max_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Try it out\n",
    "prompt = \"What is machine learning?\"\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Answer: {generate_response(prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation\n",
    "!python ../evaluation/evaluate.py \\\n",
    "    --model_path ../models/tutorial_model \\\n",
    "    --test_data ../data/processed/test.jsonl \\\n",
    "    --output_file ../results/eval_tutorial.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Try different models\n",
    "2. Use your own data\n",
    "3. Experiment with LoRA parameters\n",
    "4. Deploy with the API server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
